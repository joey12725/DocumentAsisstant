{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install nltk\n",
    "!pip install pytesseract\n",
    "!pip install pdf2image\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import openai\n",
    "import string\n",
    "import re\n",
    "import pytesseract\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "import os\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the Extractive Summary function, which identifies sentences with frequently occuring words, as well as a list of keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extractive_Summary(corpus, keywords = [], max_chars = 2500):\n",
    "    print(\"starting extractive summary\")\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(corpus)\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "    sentences = corpus.replace(\"!\", \".\").replace(\"?\", \".\").split(\".\")\n",
    "    print(\"Generated \" + str(len(sentences)) + \" sentences\")\n",
    "    sentenceValue = dict()\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "    # Average value of a sentence from the original text\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "    print(\"Average: \" + str(average))\n",
    "    print(\"Generating summary...\")\n",
    "    # Storing sentences into our summary.\n",
    "    summary = []\n",
    "    ct = 0\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue):\n",
    "            summary.append({\"sentence\": sentence, \"value\": sentenceValue[sentence], \"index\": ct})\n",
    "            ct += 1\n",
    "    ct = 0\n",
    "    summary.sort(key=lambda x: x[\"value\"], reverse=False)\n",
    "    summary_str = \"\"\n",
    "    i=0\n",
    "    print(\"Part 2 of summary generation...\")\n",
    "    summary_sentence_list = []\n",
    "    for i in summary:\n",
    "        if any(ele in i[\"sentence\"] for ele in keywords):\n",
    "            #print(\"keyword found\")\n",
    "            ct += 1\n",
    "            summary_sentence_list.append(i)\n",
    "    for i in summary:\n",
    "        #print(\"ssl: \", json.dumps(summary_sentence_list))\n",
    "        if (i not in summary_sentence_list) and (sum(len(s[\"sentence\"]) for s in summary_sentence_list) < max_chars):\n",
    "            #print(\"Sentence added\")\n",
    "            ct += 1\n",
    "            summary_sentence_list.append(i)\n",
    "    summary_sentence_list.sort(key=lambda x: x[\"index\"], reverse=False)\n",
    "    for i in summary_sentence_list:\n",
    "        summary_str += i[\"sentence\"] + \"/n\"\n",
    "    #print(\"Percent reduction: \" + str(len(summary_str)/len(data)) + \"%\")\n",
    "    print(\"Sentences in summary: \" + str(ct))\n",
    "    return summary_str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installs dependenceis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joeya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joeya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\joeya\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates corpus.txt from a pdf. Only use this if you are not manually creating corpus.txt and wish to create it from a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF to images...\n",
      "Converting images to text...\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "def PDF_OCR(filename):\n",
    "    pages = convert_from_path(os.getcwd() +\"/files/\" + filename, 500)\n",
    "    print(\"Converting PDF to images...\")\n",
    "    for i in range(len(pages)):\n",
    "        pages[i].save(os.getcwd() + '/images/page'+str(i)+'.jpg', 'JPEG')\n",
    "    print(\"Converting images to text...\")\n",
    "    text = \"\"\n",
    "    for i in range(len(pages)):\n",
    "        text += pytesseract.image_to_string(Image.open(os.getcwd() + '/images/page'+str(i)+'.jpg'))\n",
    "    print(\"Cleaning up...\")\n",
    "    for i in range(len(pages)):\n",
    "        os.remove(os.getcwd() + '/images/page'+str(i)+'.jpg')\n",
    "    return text\n",
    "\n",
    "#write text to file\n",
    "with open(\"corpus.txt\", \"w\") as f:\n",
    "    f.write(PDF_OCR(input(\"enter filename: \")))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"corpus.txt\", \"r\")\n",
    "data = text_file.read()\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary generation based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated 20.76196408180214% reduction\n",
      "That input will cost an estimated $0.05\n",
      "10000 1500\n",
      "total cost: $0.0575\n",
      "Press q to stop adding keywords\n",
      "Generating summary...\n",
      "starting extractive summary\n",
      "Generated 391 sentences\n",
      "Average: 968\n",
      "Generating summary...\n",
      "Part 2 of summary generation...\n",
      "Sentences in summary: 110\n"
     ]
    }
   ],
   "source": [
    "input_length = input(\"How long do you want the summary to be (in characters)? The corpus is \" + str(len(data)) + \" characters long. \")\n",
    "print(\"estimated \" + str(int(input_length)/len(data)*100) + \"% reduction\")\n",
    "print(\"That input will cost an estimated $\" + str(0.02*int(input_length)/4000))\n",
    "output_length = int(input(\"How long do you want your answers to br at maximum? (in characters), if you do not want to limit, type n, otherwise enter a number smaller than \" + str(15000 - int(input_length)))) \n",
    "print(input_length, output_length)\n",
    "print(\"total cost: $\" + str((output_length*0.02)/4000 + (int(input_length)*0.02)/4000))\n",
    "keywords = []\n",
    "word = \"\"\n",
    "print(\"Press q to stop adding keywords\")\n",
    "while keywords != \"q\":\n",
    "    word = input(\"Enter a keyword: \")\n",
    "    if word == \"q\":\n",
    "        break\n",
    "    keywords.append(word)\n",
    "print(\"Generating summary...\")\n",
    "summary = Extractive_Summary(data, max_chars = int(input_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = #Your API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts user for questions, and retrieves response from OpenAI api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how does moksha connect to this text?\n",
      "\n",
      "\n",
      "Moksha is mentioned in the Bhagavad Gita in connection with the idea of renunciation. Krishna tells Arjuna that renunciation is difficult to attain without discipline, and that a sage armed with discipline soon reaches the infinite spirit. He also says that those who find fault and fail to follow his thought are lost fools, deluded by every bit of knowledge. Therefore, moksha could be seen as a state of clarity or enlightenment that is attained through discipline and renunciation.\n",
      "total cost: $0.066\n"
     ]
    }
   ],
   "source": [
    "question = input(\"Enter your question: \")\n",
    "prompt = summary + \"\\n\\n\\n Based on this passage, \" + question\n",
    "temp = int(input(\"On a scale of 1 to 10, how creative would you like your response to be\"))/10\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-002\",\n",
    "  prompt=prompt,\n",
    "  temperature=temp,\n",
    "  max_tokens=int(output_length/4),\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(question)\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "print(\"total cost: $\" + str((response[\"usage\"][\"total_tokens\"]/1000)*0.02))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad7b8688808683e371c23e4b0bc5004c046d67ef397b56976e925739a67083ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
